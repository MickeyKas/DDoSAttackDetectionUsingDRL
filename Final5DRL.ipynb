{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f19dc8b2-4fbe-408b-a98f-c1a082dfb5a6",
   "metadata": {},
   "source": [
    "\n",
    "# PROJECT OVERVIEW\n",
    "\n",
    "## Title: Real-Time DDoS Detection in Industrial IIoT Using Deep Reinforcement Learning (DRL) Algorithms\n",
    "\n",
    "This notebook presents a complete, end-to-end pipeline for detecting Distributed Denial of Service (DDoS) attacks in Industrial Internet of Things (IIoT) networks using Deep Reinforcement Learning (DRL). The models are ested on ToN-IoT KDDCup99, CIC-DDoS2019, and Edge-IIoT Datasets. PPO algorithm based model achieved  high accuracy on all three datasets as well as near zero false positives and negatives, inference < 0.23 ms per sample, ONNX export removing PyTorch dependency in production, and full reproducibility, Unified feature engineering across datasets. Comprehensive visualizations including accuracy, latency, confusion matrix, convergence have also been tested with all datasets. 5 DRL techniques including PPO, DQN, DoubleDQN, Dueling DQN, DDPG have been tested and PPO outperformed all of them. \n",
    "\n",
    "### Datasets: KDDCup99, CIC-DDoS2019, Edge-IIoT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b798ef2-ec1f-4de5-9a23-3a7f2ff65619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2: LIBRARIES & REPRODUCIBILITY\n",
    "import warnings, os, random, gc, time, psutil\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "random.seed(42)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import os\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# sklearn imports – all needed ones consolidated\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler  # <-- Added RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "import joblib\n",
    "import onnxruntime as ort\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"All libraries imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595af0ec-88da-4bf3-a582-b3c554b2ce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3: MULTI-DATASET LOADING\n",
    "\n",
    "print(\"CELL 3: Loading all 3 datasets...\")\n",
    "\n",
    "# WORKING BASE PATH FROM THE OTHER PROJECT\n",
    "base_path = Path(r\"F:\\jupyter\\kagglehub\")\n",
    "\n",
    "# PATH STRUCTURE –  POINTING TO THE REAL LOCATION\n",
    "paths = {\n",
    "    'kddcup99': base_path / r\"datasets\\ericzs\\kddcup99\\versions\",\n",
    "    'cic_ddos': base_path / r\"datasets\\dhoogla\\cicddos2019\\versions\\3\",\n",
    "    'edge_iot': base_path / r\"edgeiiotset-cyber-security-dataset-of-iot-iiot\\versions\\5\\Edge-IIoTset dataset\\Selected dataset for ML and DL\"\n",
    "}\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "def map_to_binary(label):\n",
    "    if pd.isna(label): \n",
    "        return 0\n",
    "    s = str(label).lower().replace('_', '').replace(' ', '').replace('-', '')\n",
    "    return 0 if any(k in s for k in ['normal', 'benign', '0']) else 1\n",
    "\n",
    "for name, root in paths.items():\n",
    "    print(f\"\\nLoading {name.upper()} from: {root}\")\n",
    "    \n",
    "    if not root.exists():\n",
    "        raise FileNotFoundError(f\"Path not found: {root}\")\n",
    "    \n",
    "    files = list(root.rglob(\"*.csv\")) + list(root.rglob(\"*.parquet\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No CSV/PARQUET files in {root}\")\n",
    "    \n",
    "    print(f\"   Found {len(files)} file(s)\")\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        try:\n",
    "            if f.suffix == '.parquet':\n",
    "                df_part = pd.read_parquet(f)\n",
    "            else:\n",
    "                df_part = pd.read_csv(f, low_memory=False)\n",
    "            print(f\"   → Loaded {f.name}: {len(df_part):,} rows\")\n",
    "            dfs.append(df_part)\n",
    "            del df_part\n",
    "            gc.collect()\n",
    "        except Exception as e:\n",
    "            print(f\"   ! Failed {f.name}: {e}\")\n",
    "    \n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"   Total rows: {len(df):,}\")\n",
    "    \n",
    "    # Auto-detect label column\n",
    "    label_col = next((c for c in ['Label', 'label', 'Attack_type', 'Attack', 'class'] if c in df.columns), df.columns[-1])\n",
    "    print(f\"   Using label column: '{label_col}'\")\n",
    "    \n",
    "    df['target'] = df[label_col].apply(map_to_binary)\n",
    "    print(f\"   Attack ratio (raw): {df['target'].mean():.4%}\")\n",
    "    \n",
    "    X_raw = df.select_dtypes(include=np.number).drop(columns=[label_col], errors='ignore')\n",
    "    X = np.nan_to_num(X_raw.values, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "    y = df['target'].values.astype(np.int64)\n",
    "    \n",
    "    # Downsample KDD & CIC to ~30% attacks (realistic, prevents trivial accuracy)\n",
    "    if name != 'edge_iot':\n",
    "        idx_normal = np.where(y == 0)[0]\n",
    "        idx_attack = np.where(y == 1)[0]\n",
    "        n_normal = len(idx_normal)\n",
    "        n_attack_needed = int(n_normal * 0.3 / 0.7)  # ~30% attacks\n",
    "        idx_attack = np.random.choice(idx_attack, min(n_attack_needed, len(idx_attack)), replace=False)\n",
    "        idx = np.concatenate([idx_normal, idx_attack])\n",
    "        np.random.shuffle(idx)\n",
    "        X, y = X[idx], y[idx]\n",
    "        print(f\"   Downsampled → {len(X):,} rows | Attack ratio: {y.mean():.4%}\")\n",
    "    \n",
    "    # Train (70%) / Val (15%) / Test (15%) split\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "    \n",
    "    scaler = RobustScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val   = scaler.transform(X_val)\n",
    "    X_test  = scaler.transform(X_test)\n",
    "    \n",
    "    # Convert to torch tensors and store\n",
    "    datasets[name] = {\n",
    "        'X_train': torch.FloatTensor(X_train),\n",
    "        'X_val':   torch.FloatTensor(X_val),\n",
    "        'X_test':  torch.FloatTensor(X_test),\n",
    "        'y_train': torch.LongTensor(y_train),\n",
    "        'y_val':   torch.LongTensor(y_val),\n",
    "        'y_test':  torch.LongTensor(y_test),\n",
    "        'state_size': X_train.shape[1],\n",
    "        'scaler': scaler,\n",
    "        'features': X_raw.columns.tolist()\n",
    "    }\n",
    "    \n",
    "    print(f\"   SUCCESS → Train: {len(X_train):,} | Val: {len(X_val):,} | Test: {len(X_test):,} | Features: {datasets[name]['state_size']}\")\n",
    "    \n",
    "    del df, X, y, X_train, X_val, X_test\n",
    "    gc.collect()\n",
    "    \n",
    "print(\"All 3 datasets loaded and preprocessed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5c12a9-f5dc-4503-89fa-946941f66f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4: ENVIRONMENT, REPLAY BUFFER & NETWORKS - SELF-CONTAINED\n",
    "\n",
    "class DDoSEnv:\n",
    "    def __init__(self, states, labels):\n",
    "        self.states = states\n",
    "        self.labels = labels\n",
    "        self.n = len(states)\n",
    "    \n",
    "    def reset(self):\n",
    "        idx = random.randint(0, self.n - 1)\n",
    "        return self.states[idx].clone(), idx\n",
    "    \n",
    "    def step(self, action, true_label):\n",
    "        reward = 1.0 if int(action) == true_label else -1.0\n",
    "        done = True\n",
    "        next_idx = random.randint(0, self.n - 1)\n",
    "        return self.states[next_idx].clone(), reward, done\n",
    "\n",
    "class PrioritizedReplay:\n",
    "    def __init__(self, capacity=500_000, alpha=0.6, beta=0.4):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.buffer = []\n",
    "        self.priorities = []\n",
    "        self.pos = 0\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        priority = max(self.priorities, default=1.0)\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "            self.priorities.append(None)\n",
    "        self.buffer[self.pos] = (state.clone().detach(), torch.tensor(action), \n",
    "                                 torch.tensor(reward), next_state.clone().detach(), torch.tensor(done))\n",
    "        self.priorities[self.pos] = priority\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        priorities = np.array(self.priorities[:len(self.buffer)], dtype=np.float64)\n",
    "        probs = priorities ** self.alpha\n",
    "        probs /= probs.sum() + 1e-8\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[i] for i in indices]\n",
    "        weights = (len(self.buffer) * probs[indices]) ** (-self.beta)\n",
    "        weights /= weights.max() + 1e-8\n",
    "        batch = tuple(torch.stack(t) for t in zip(*samples))\n",
    "        return batch, torch.FloatTensor(weights), indices\n",
    "    \n",
    "    def update_priorities(self, indices, priorities):\n",
    "        for idx, p in zip(indices, priorities):\n",
    "            self.priorities[idx] = p + 1e-6\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DuelingQNetwork(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 256), nn.ReLU()\n",
    "        )\n",
    "        self.value = nn.Linear(256, 1)\n",
    "        self.advantage = nn.Linear(256, 2)\n",
    "    def forward(self, x):\n",
    "        f = self.feature(x)\n",
    "        v = self.value(f)\n",
    "        a = self.advantage(f)\n",
    "        return v + (a - a.mean(dim=1, keepdim=True))\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256), nn.Tanh(),\n",
    "            nn.Linear(256, 256), nn.Tanh()\n",
    "        )\n",
    "        self.actor = nn.Linear(256, 2)\n",
    "        self.critic = nn.Linear(256, 1)\n",
    "    def forward(self, x):\n",
    "        f = self.shared(x)\n",
    "        return self.actor(f), self.critic(f)\n",
    "\n",
    "print(\"Environment, Prioritized Replay, and all 5 network architectures ready\")\n",
    "print(\"Compatible with different state sizes from KDDCup99 (39), CIC-DDoS (78), Edge-IIoT (44)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad35df8e-8267-47b4-a024-cc49915d12cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5: TRAINING LOOP - 5 MODELS: DQN, DoubleDQN, Duelling DQN, PPO, DDPG\n",
    "# Progress shown per dataset and per agent\n",
    "\n",
    "results = []\n",
    "trained_models = {name: {} for name in datasets.keys()}\n",
    "convergence_logs = {name: {} for name in datasets.keys()}\n",
    "\n",
    "# DDPG networks , adapted for binary action\n",
    "class DDPGActor(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 1), nn.Sigmoid()  # Probability of attack (1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DDPGCritic(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim + 1, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    def forward(self, state, action):\n",
    "        return self.net(torch.cat([state, action], dim=1))\n",
    "\n",
    "def train_dqn_variant(dataset_name, agent_name, use_dueling=False, use_double=False):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRAINING {agent_name.upper()} ON {dataset_name.upper()}\")\n",
    "    print(f\"Features: {datasets[dataset_name]['state_size']} | Train samples: {len(datasets[dataset_name]['X_train']):,}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    data = datasets[dataset_name]\n",
    "    state_size = data['state_size']\n",
    "    env = DDoSEnv(data['X_train'], data['y_train'])\n",
    "    replay = PrioritizedReplay()\n",
    "    \n",
    "    model = DuelingQNetwork(state_size) if use_dueling else QNetwork(state_size)\n",
    "    target = DuelingQNetwork(state_size) if use_dueling else QNetwork(state_size)\n",
    "    target.load_state_dict(model.state_dict())\n",
    "    target.eval()\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    eps = 1.0\n",
    "    episodes = 3000\n",
    "    batch_size = 256\n",
    "    \n",
    "    accuracies = deque(maxlen=1000)\n",
    "    conv_log = []\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        state, idx = env.reset()\n",
    "        label = int(data['y_train'][idx])\n",
    "        if random.random() < eps:\n",
    "            action = random.randint(0, 1)\n",
    "        else:\n",
    "            action = model(state.unsqueeze(0)).argmax().item()\n",
    "        \n",
    "        next_state, reward, done = env.step(action, label)\n",
    "        replay.push(state, action, reward, next_state, done)\n",
    "        accuracies.append(action == label)\n",
    "        \n",
    "        if len(replay) > batch_size:\n",
    "            batch, weights, indices = replay.sample(batch_size)\n",
    "            s, a, r, ns, d = batch\n",
    "            current_q = model(s).gather(1, a.unsqueeze(1)).squeeze()\n",
    "            with torch.no_grad():\n",
    "                if use_double:\n",
    "                    next_a = model(ns).argmax(1)\n",
    "                    next_q = target(ns).gather(1, next_a.unsqueeze(1)).squeeze()\n",
    "                else:\n",
    "                    next_q = target(ns).max(1)[0]\n",
    "                target_q = r + 0.99 * next_q * (1.0 - d.float())\n",
    "            td_error = current_q - target_q\n",
    "            loss = (td_error.pow(2) * weights).mean()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            replay.update_priorities(indices, td_error.abs().detach().cpu().numpy())\n",
    "        \n",
    "        if ep % 50 == 0 and ep > 0:\n",
    "            target.load_state_dict(model.state_dict())\n",
    "        \n",
    "        eps = max(0.05, eps * 0.999)\n",
    "        \n",
    "        if (ep + 1) % 500 == 0:\n",
    "            acc = np.mean(accuracies) * 100\n",
    "            conv_log.append((ep + 1, acc))\n",
    "            print(f\"   Episode {ep+1:4d} | Rolling Accuracy: {acc:6.2f}%\")\n",
    "    \n",
    "    final_acc = np.mean(accuracies) * 100\n",
    "    results.append({'Dataset': dataset_name.upper(), 'Agent': agent_name, 'Accuracy (%)': round(final_acc, 3)})\n",
    "    convergence_logs[dataset_name][agent_name] = conv_log\n",
    "    trained_models[dataset_name][agent_name] = model\n",
    "    print(f\"\\n{agent_name.upper()} COMPLETED → Final Accuracy: {final_acc:.3f}% on {dataset_name.upper()}\")\n",
    "\n",
    "def train_ppo(dataset_name):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRAINING PPO ON {dataset_name.upper()}\")\n",
    "    print(f\"Features: {datasets[dataset_name]['state_size']} | Train samples: {len(datasets[dataset_name]['X_train']):,}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    data = datasets[dataset_name]\n",
    "    state_size = data['state_size']\n",
    "    env = DDoSEnv(data['X_train'], data['y_train'])\n",
    "    model = ActorCritic(state_size)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "    episodes = 3000\n",
    "    accuracies = deque(maxlen=1000)\n",
    "    conv_log = []\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        state, idx = env.reset()\n",
    "        label = int(data['y_train'][idx])\n",
    "        logits, value = model(state.unsqueeze(0))\n",
    "        dist = Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        _, reward, _ = env.step(action.item(), label)\n",
    "        accuracies.append(action.item() == label)\n",
    "        \n",
    "        advantage = reward - value.item()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        ratio = torch.exp(log_prob - log_prob.detach())\n",
    "        clipped = torch.clamp(ratio, 0.8, 1.2)\n",
    "        loss = -torch.min(ratio * advantage, clipped * advantage).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (ep + 1) % 500 == 0:\n",
    "            acc = np.mean(accuracies) * 100\n",
    "            conv_log.append((ep + 1, acc))\n",
    "            print(f\"   Episode {ep+1:4d} | Rolling Accuracy: {acc:6.2f}%\")\n",
    "    \n",
    "    final_acc = np.mean(accuracies) * 100\n",
    "    results.append({'Dataset': dataset_name.upper(), 'Agent': 'PPO', 'Accuracy (%)': round(final_acc, 3)})\n",
    "    convergence_logs[dataset_name]['PPO'] = conv_log\n",
    "    trained_models[dataset_name]['PPO'] = model\n",
    "    print(f\"\\nPPO COMPLETED → Final Accuracy: {final_acc:.3f}% on {dataset_name.upper()}\")\n",
    "\n",
    "def train_ddpg(dataset_name):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRAINING DDPG ON {dataset_name.upper()}\")\n",
    "    print(f\"Features: {datasets[dataset_name]['state_size']} | Train samples: {len(datasets[dataset_name]['X_train']):,}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    data = datasets[dataset_name]\n",
    "    state_size = data['state_size']\n",
    "    env = DDoSEnv(data['X_train'], data['y_train'])\n",
    "    \n",
    "    actor = DDPGActor(state_size)\n",
    "    critic = DDPGCritic(state_size)\n",
    "    actor_optimizer = optim.Adam(actor.parameters(), lr=1e-4)\n",
    "    critic_optimizer = optim.Adam(critic.parameters(), lr=1e-3)\n",
    "    \n",
    "    episodes = 3000\n",
    "    accuracies = deque(maxlen=1000)\n",
    "    conv_log = []\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        state, idx = env.reset()\n",
    "        label = int(data['y_train'][idx])\n",
    "        \n",
    "        prob = actor(state.unsqueeze(0))\n",
    "        action = (prob > 0.5).float().item()  # Binary decision\n",
    "        \n",
    "        _, reward, _ = env.step(action, label)\n",
    "        accuracies.append(action == label)\n",
    "        \n",
    "        # Simple DDPG update (actor maximizes critic, critic minimizes TD)\n",
    "        q_value = critic(state.unsqueeze(0), torch.tensor([[action]], dtype=torch.float32))\n",
    "        critic_loss = F.mse_loss(q_value, torch.tensor([[reward]], dtype=torch.float32))\n",
    "        critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        critic_optimizer.step()\n",
    "        \n",
    "        actor_loss = -critic(state.unsqueeze(0), prob).mean()\n",
    "        actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        actor_optimizer.step()\n",
    "        \n",
    "        if (ep + 1) % 500 == 0:\n",
    "            acc = np.mean(accuracies) * 100\n",
    "            conv_log.append((ep + 1, acc))\n",
    "            print(f\"   Episode {ep+1:4d} | Rolling Accuracy: {acc:6.2f}%\")\n",
    "    \n",
    "    final_acc = np.mean(accuracies) * 100\n",
    "    results.append({'Dataset': dataset_name.upper(), 'Agent': 'DDPG', 'Accuracy (%)': round(final_acc, 3)})\n",
    "    convergence_logs[dataset_name]['DDPG'] = conv_log\n",
    "    trained_models[dataset_name]['DDPG'] = actor\n",
    "    print(f\"\\nDDPG COMPLETED → Final Accuracy: {final_acc:.3f}% on {dataset_name.upper()}\")\n",
    "\n",
    "# Train ALL 5 MODELS on ALL 3 DATASETS\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TRAINING OF ALL 5 DRL MODELS ON ALL 3 DATASETS\")\n",
    "print(\"Models: DQN, DoubleDQN, Dueling DQN, PPO, DDPG\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for ds_name in datasets.keys():\n",
    "    train_dqn_variant(ds_name, \"DQN\")\n",
    "    train_dqn_variant(ds_name, \"DoubleDQN\", use_double=True)\n",
    "    train_dqn_variant(ds_name, \"Dueling\", use_dueling=True)\n",
    "    train_ppo(ds_name)\n",
    "    train_ddpg(ds_name)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL 5 MODELS TRAINED ON ALL 3 DATASETS\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03c8a7b-4f3f-4792-a37a-ebc7ce84ee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6: RESULTS TABLE, Display results\n",
    "\n",
    "\n",
    "# Creating DataFrame from results which is collected during training\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "df_pivot = df_results.pivot(index='Dataset', columns='Agent', values='Accuracy (%)')\n",
    "\n",
    "# Datasets alphabetically, and agents alphabetically\n",
    "df_pivot = df_pivot.sort_index()  # Datasets alphabetical\n",
    "df_pivot = df_pivot[sorted(df_pivot.columns)]  # Agents alphabetical\n",
    "\n",
    "# Round for clean display\n",
    "df_pivot = df_pivot.round(3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL ACCURACY RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(df_pivot.to_string())\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Saving dynamically\n",
    "df_pivot.to_csv(\"drl_results_pivoted_dynamic.csv\")\n",
    "print(f\"\\nPivoted results saved to 'drl_results_pivoted_dynamic.csv'\")\n",
    "\n",
    "# best model per dataset\n",
    "print(\"\\nBEST MODEL PER DATASET:\")\n",
    "for dataset in df_pivot.index:\n",
    "    best_agent = df_pivot.loc[dataset].idxmax()\n",
    "    best_acc = df_pivot.loc[dataset].max()\n",
    "    print(f\"   {dataset}: {best_agent} → {best_acc:.3f}% accuracy\")\n",
    "\n",
    "print(\"Performance across all datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475b024b-9bb1-4eef-b8ea-701979fc7d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7: CONFUSION MATRICES AND DETAILED METRICS \n",
    "\n",
    "\n",
    "# Creating figures folder and save path\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "figure_path = \"figures/confusion_matrices_all.png\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONFUSION MATRICES AS A SINGLE FIGURE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "fig, axes = plt.subplots(5, 3, figsize=(15, 20))  # 5 rows for models and 3 columns for datasets\n",
    "fig.suptitle('Confusion Matrices', \n",
    "             fontsize=18, fontweight='bold', y=0.98)\n",
    "\n",
    "model_order = ['DQN', 'DoubleDQN', 'Dueling', 'PPO', 'DDPG']\n",
    "dataset_order = sorted(datasets.keys())\n",
    "\n",
    "def annotate_cm(ax, cm):\n",
    "    total = cm.sum()\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            count = cm[i, j]\n",
    "            percent = 100 * count / total if total > 0 else 0\n",
    "            text = f\"{count}\\n({percent:.1f}%)\"\n",
    "            ax.text(j+0.5, i+0.5, text,\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    fontsize=12, fontweight='bold',\n",
    "                    color=\"black\" if count < total/2 else \"white\")\n",
    "\n",
    "# Storing metrics for printing\n",
    "metrics_per_model = {agent: {} for agent in model_order}\n",
    "\n",
    "for row_idx, agent in enumerate(model_order):\n",
    "    axes[row_idx, 0].text(-0.4, 0.5, agent, rotation=90, fontsize=14, fontweight='bold',\n",
    "                          va='center', ha='center', transform=axes[row_idx, 0].transAxes)\n",
    "    \n",
    "    for col_idx, ds_name in enumerate(dataset_order):\n",
    "        ax = axes[row_idx, col_idx]\n",
    "        data = datasets[ds_name]\n",
    "        true_labels = data['y_test'].numpy()\n",
    "        \n",
    "        model = trained_models[ds_name][agent]\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            if agent == 'PPO':\n",
    "                logits, _ = model(data['X_test'])\n",
    "                preds = logits.argmax(dim=1).cpu().numpy()\n",
    "            elif agent == 'DDPG':\n",
    "                prob = model(data['X_test'])\n",
    "                preds = (prob > 0.5).cpu().numpy().flatten().astype(int)\n",
    "            else:\n",
    "                preds = model(data['X_test']).argmax(dim=1).cpu().numpy()\n",
    "        \n",
    "        cm = confusion_matrix(true_labels, preds)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        total = cm.sum()\n",
    "        acc = 100 * (tp + tn) / total if total > 0 else 0\n",
    "        prec = 100 * tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = 100 * tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 100 * 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0\n",
    "        fnr = 100 * fn / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        \n",
    "        metrics_per_model[agent][ds_name.upper()] = {\n",
    "            'ACC (%)': round(acc, 2),\n",
    "            'Precision (%)': round(prec, 2),\n",
    "            'Recall (%)': round(recall, 2),\n",
    "            'F1 (%)': round(f1, 2),\n",
    "            'FNR (%)': round(fnr, 2)\n",
    "        }\n",
    "        \n",
    "        sns.heatmap(cm, annot=False, cmap='Blues', ax=ax, cbar=False,\n",
    "                    linewidths=1.5, linecolor='white')\n",
    "        annotate_cm(ax, cm)\n",
    "        \n",
    "        if row_idx == 0:\n",
    "            ax.set_title(ds_name.upper(), fontsize=13, fontweight='bold')\n",
    "        \n",
    "        ax.set_xlabel('Predicted' if row_idx == 4 else '')\n",
    "        ax.set_ylabel('True' if col_idx == 0 else '')\n",
    "        ax.set_xticklabels(['Normal', 'Attack'] if row_idx == 4 else ['', ''])\n",
    "        ax.set_yticklabels(['Normal', 'Attack'] if col_idx == 0 else ['', ''])\n",
    "\n",
    "plt.tight_layout(rect=[0.07, 0, 1, 0.95])\n",
    "fig.savefig(figure_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFigure saved to: {figure_path}\")\n",
    "\n",
    "# Printing calculations grouped by model\n",
    "print(\"\\n\" + \"=\"*160)\n",
    "print(\"DETAILED METRICS PER MODEL AND DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for agent in model_order:\n",
    "    print(f\"\\n{agent.upper()}:\")\n",
    "    df_agent = pd.DataFrame(metrics_per_model[agent]).T\n",
    "    df_agent = df_agent[['ACC (%)', 'Precision (%)', 'Recall (%)', 'F1 (%)', 'FNR (%)']]\n",
    "    print(df_agent.to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Single figure is saved as PNG in the 'figures' folder\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6621c3-f9f2-4e0b-b829-c9297ca2135a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8: FINAL BENCHMARK; ONE single tall figure with 3 subplots stacked vertically, one per dataset\n",
    "\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "figure_path = \"figures/accuracy_latency_all_datasets.png\"\n",
    "\n",
    "process = psutil.Process()\n",
    "\n",
    "def measure_latency(model, test_tensor, n_samples=5000):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        size = min(n_samples, test_tensor.shape[0])\n",
    "        indices = torch.randint(0, test_tensor.shape[0], (size,))\n",
    "        samples = test_tensor[indices]\n",
    "        start = time.perf_counter()\n",
    "        for sample in samples:\n",
    "            if 'DDPG' in str(type(model)):\n",
    "                _ = model(sample.unsqueeze(0))\n",
    "            elif 'ActorCritic' in str(type(model)):\n",
    "                _ = model(sample.unsqueeze(0))[0]\n",
    "            else:\n",
    "                _ = model(sample.unsqueeze(0))\n",
    "        elapsed = time.perf_counter() - start\n",
    "    return elapsed / size * 1000  # ms per inference\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INFERENCE BENCHMARK; SINGLE VERTICAL FIGURE & CALCULATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Collectig benchmark data\n",
    "benchmark_data = []\n",
    "for ds_name in sorted(datasets.keys()):\n",
    "    data = datasets[ds_name]\n",
    "    for agent in sorted(trained_models[ds_name].keys()):\n",
    "        model = trained_models[ds_name][agent]\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            if agent == 'DDPG':\n",
    "                prob = model(data['X_test'])\n",
    "                preds = (prob > 0.5).cpu().numpy().flatten().astype(int)\n",
    "            elif agent == 'PPO':\n",
    "                logits, _ = model(data['X_test'])\n",
    "                preds = logits.argmax(dim=1).cpu().numpy()\n",
    "            else:\n",
    "                preds = model(data['X_test']).argmax(dim=1).cpu().numpy()\n",
    "        acc = accuracy_score(data['y_test'].numpy(), preds) * 100\n",
    "        lat = measure_latency(model, data['X_test'])\n",
    "        benchmark_data.append({\n",
    "            'Dataset': ds_name.upper(),\n",
    "            'Agent': agent,\n",
    "            'Accuracy (%)': round(acc, 3),\n",
    "            'Latency (ms)': round(lat, 4)\n",
    "        })\n",
    "\n",
    "# Creating single vertical figure\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 18))  # 3 rows vertical, 1 column\n",
    "fig.suptitle('Accuracy and Latency Across Datasets', fontsize=18, fontweight='bold', y=0.98)\n",
    "\n",
    "dataset_order = sorted(datasets.keys())\n",
    "agents_order = sorted(trained_models[dataset_order[0]].keys())  # Consistent agent order\n",
    "\n",
    "for row_idx, ds_name in enumerate(dataset_order):\n",
    "    sub_df = pd.DataFrame([d for d in benchmark_data if d['Dataset'] == ds_name.upper()])\n",
    "    sub_df = sub_df.set_index('Agent').reindex(agents_order).reset_index()  # Consistent order\n",
    "    \n",
    "    x = np.arange(len(sub_df))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax1 = axes[row_idx]\n",
    "    bars1 = ax1.bar(x - width/2, sub_df['Accuracy (%)'], width,\n",
    "                    label='Accuracy (%)', color='tab:green', alpha=0.9, edgecolor='black')\n",
    "    ax1.set_ylabel('Accuracy (%)', color='tab:green', fontsize=12)\n",
    "    ax1.set_ylim(50, 101)\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:green')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(sub_df['Agent'], fontsize=11)\n",
    "    ax1.set_title(f'{ds_name.upper()} Dataset', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    bars2 = ax2.bar(x + width/2, sub_df['Latency (ms)'], width,\n",
    "                    label='Latency (ms)', color='tab:red', alpha=0.8, edgecolor='black')\n",
    "    ax2.set_ylabel('Latency (ms)', color='tab:red', fontsize=12)\n",
    "    max_lat = sub_df['Latency (ms)'].max()\n",
    "    ax2.set_ylim(0, max_lat * 1.4)\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "    \n",
    "    # Value labels\n",
    "    for bar in bars1:\n",
    "        h = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., h + 1,\n",
    "                 f'{h:.1f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    for bar in bars2:\n",
    "        h = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., h + (max_lat * 0.02),\n",
    "                 f'{h:.3f}', ha='center', va='bottom', fontsize=9, color='darkred')\n",
    "\n",
    "handles, labels = ax1.get_legend_handles_labels()\n",
    "handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "fig.legend(handles + handles2, labels + labels2,\n",
    "           loc='upper center', bbox_to_anchor=(0.5, 0.95), ncol=2, fontsize=12)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.93])\n",
    "fig.savefig(figure_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSingle vertical figure saved: {figure_path}\")\n",
    "\n",
    "# Calculations at bottom\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"DETAILED BENCHMARK TABLE FOR ALL DATASETS\")\n",
    "print(\"=\"*90)\n",
    "df_all = pd.DataFrame(benchmark_data)\n",
    "df_pivot = df_all.pivot(index='Agent', columns='Dataset', values=['Accuracy (%)', 'Latency (ms)'])\n",
    "df_pivot = df_pivot.sort_index(axis=1, level='Dataset')  # Sort datasets\n",
    "print(df_pivot.to_string())\n",
    "\n",
    "# Summary calculations\n",
    "print(\"\\nSUMMARY CALCULATIONS:\")\n",
    "mean_acc = df_all.groupby('Agent')['Accuracy (%)'].mean().round(2)\n",
    "print(\"Mean Accuracy across datasets:\")\n",
    "print(mean_acc.to_string())\n",
    "\n",
    "best_acc = df_all.loc[df_all['Accuracy (%)'].idxmax()]\n",
    "print(f\"\\nBest overall accuracy: {best_acc['Agent']} on {best_acc['Dataset']} ({best_acc['Accuracy (%)']:.3f}%)\")\n",
    "\n",
    "print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d503b6-d5d6-4358-9570-1a57d02ec0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9:AUC-ROC:  for all 5 models on all 3 datasets\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AUC-ROC\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. AUC-ROC Calculation\n",
    "print(\"\\nAUC-ROC Scores\")\n",
    "auc_results = []\n",
    "\n",
    "for ds_name in sorted(datasets.keys()):\n",
    "    data = datasets[ds_name]\n",
    "    true_labels = data['y_test'].numpy()\n",
    "    print(f\"\\n{ds_name.upper()} Dataset:\")\n",
    "    for agent in sorted(trained_models[ds_name].keys()):\n",
    "        model = trained_models[ds_name][agent]\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            if agent == 'PPO':\n",
    "                logits, _ = model(data['X_test'])\n",
    "                probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
    "            elif agent == 'DDPG':\n",
    "                probs_raw = model(data['X_test']).squeeze().cpu().numpy()\n",
    "                # Normalize to [0,1] for valid probability\n",
    "                probs = (probs_raw - probs_raw.min()) / (probs_raw.max() - probs_raw.min() + 1e-8)\n",
    "            else:\n",
    "                logits = model(data['X_test'])\n",
    "                probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
    "        auc = roc_auc_score(true_labels, probs)\n",
    "        auc_results.append({'Dataset': ds_name.upper(), 'Agent': agent, 'AUC-ROC': round(auc, 4)})\n",
    "        print(f\"   {agent:10}: {auc:.4f}\")\n",
    "\n",
    "# Pivoted AUC table \n",
    "df_auc = pd.DataFrame(auc_results)\n",
    "pivot_auc = df_auc.pivot(index='Agent', columns='Dataset', values='AUC-ROC')\n",
    "pivot_auc = pivot_auc[['KDDCUP99', 'CIC_DDOS', 'EDGE_IOT']] \n",
    "\n",
    "print(\"\\nAUC-ROC Table (Models as rows, Datasets as columns):\")\n",
    "print(pivot_auc.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683c11f0-c959-491a-bfda-154a319a547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 10: ONNX EXPORT, BEST PPO MODEL PER DATASET EXPORTED\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "class PPOForONNX(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super().__init__()\n",
    "        self.model = original_model\n",
    "    def forward(self, x):\n",
    "        logits, _ = self.model(x)\n",
    "        return logits\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPORTING BEST PPO MODELS TO ONNX - ONE PER DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for ds_name in datasets.keys():\n",
    "    if 'PPO' not in trained_models[ds_name]:\n",
    "        print(f\"   PPO not found for {ds_name.upper()} – skipping\")\n",
    "        continue\n",
    "    \n",
    "    ppo_model = trained_models[ds_name]['PPO']\n",
    "    ppo_model.eval()\n",
    "    \n",
    "    state_size = datasets[ds_name]['state_size']\n",
    "    wrapped = PPOForONNX(ppo_model)\n",
    "    \n",
    "    dummy_input = torch.randn(1, state_size)\n",
    "    \n",
    "    onnx_filename = f\"models/PPO_DDoS_{ds_name.upper()}.onnx\"\n",
    "    torch.onnx.export(\n",
    "        wrapped,\n",
    "        dummy_input,\n",
    "        onnx_filename,\n",
    "        export_params=True,\n",
    "        opset_version=18,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['input_features'],\n",
    "        output_names=['action_logits'],\n",
    "        dynamic_axes={'input_features': {0: 'batch'}, 'action_logits': {0: 'batch'}}\n",
    "    )\n",
    "    \n",
    "    print(f\"   Exported: {onnx_filename} | Features: {state_size}\")\n",
    "\n",
    "print(\"\\nONNX export complete\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1401204-7599-4ddd-99da-1d8e96c1c178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f1fce7-e33d-4a7c-9a36-baf156936aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
